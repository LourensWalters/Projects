Classification of Coronary Artery Disease
=========================================

The main objective of the analysis was to explore the Cleveland "Coronary Artery Disease" dataset (Detrano et al., 1989)(Patel, 2020) by applying various ML techniques to better understand the information contained in the dataset, and to document any findings. The dataset used can be found on the UCI Machine Learning Repository at the following location:

[Heart Disease Dataset](https://archive.ics.uci.edu/ml/datasets/Heart+Disease "Heart Disease Dataset")

I used PyCharm for development, and the virtual environment files generated by PyCharm (and used by the project) are also uploaded to GitHub. The environment can hence be reproduced. A static version of the Notebook can be accessed [here](https://github.com/LourensWalters/Projects/blob/main/heart_disease/notebooks/explore_data_2020_10_15_lw.ipynb "here") for purposes of viewing the analysis.  

Approach
========

My approach was to build a Machine Learning (ML) model using the best practice method of: load, prepare data, build model, validate model and analyse results. My objective was to determine whether there is a strong signal in the data and whether this signal can be captured by a parsimonious model. To achieve this, I decided to use a Multilayer Perceptron (MLP) from the scikit-learn library. The MLP is a universal approximator and is hence powerful, but easy to implement and parameterise manually and is therefore ideal for fast exploratory projects. If any signals were detected these could be improved upon by subsequently testing ensemble models or further optimisation using Deep Learning with scikit-learn, Theano, TensorFlow, Keras or PyTorch. Future notebooks and code will soon be published for these analyses. 

I used the cookiecutter Machine Learning source code template for this project. This template organises code, data and documentation in a structured manner which allows for flexible exploration of data and structured deployment of production ready code.  

I focused on the exploratory part of the analysis, eliciting as much information from the data as possible. Future work would be to refactor the code for production readiness. I chose the Jupyter Notebook tool for exploration. The notebook is heavily annotated with comments on the analysis and should be used as the primary source for evaluating the analysis. I also created templates to refactor segments of the code into Object Oriented Python, as a start to this possible future process. 

I used PyCharm for development, and the virtual environment files generated by PyCharm (and used by the project) are also uploaded to GitHub. The environment can hence be reproduced. A static version of the Notebook can be accessed here for purposes of reviewing the analysis.  

Project Organization
====================

    ├── LICENSE
    ├── Makefile           <- Makefile with commands like `make data` or `make train`
    ├── README.md          <- The top-level README for developers using this project.
    ├── data
    │   ├── external       <- Data from third party sources.
    │   ├── interim        <- Intermediate data that has been transformed.
    │   ├── processed      <- The final, canonical data sets for modeling.
    │   └── raw            <- The original, immutable data dump.
    │
    ├── docs               <- A default Sphinx project; see sphinx-doc.org for details
    │
    ├── models             <- Trained and serialized models, model predictions, or model summaries
    │
    ├── notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
    │                         the creator's initials, and a short `-` delimited description, e.g.
    │                         `1.0-jqp-initial-data-exploration`.
    │
    ├── references         <- Data dictionaries, manuals, and all other explanatory materials.
    │
    ├── reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
    │   └── figures        <- Generated graphics and figures to be used in reporting
    │
    ├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
    │                         generated with `pip freeze > requirements.txt`
    │
    ├── setup.py           <- makes project pip installable (pip install -e .) so src can be imported
    ├── src                <- Source code for use in this project.
    │   ├── __init__.py    <- Makes src a Python module
    │   │
    │   ├── data           <- Scripts to download or generate data
    │   │   └── make_dataset.py
    │   │
    │   ├── features       <- Scripts to turn raw data into features for modeling
    │   │   └── build_features.py
    │   │
    │   ├── models         <- Scripts to train models and then use trained models to make
    │   │   │                 predictions
    │   │   ├── predict_model.py
    │   │   └── train_model.py
    │   │
    │   └── visualization  <- Scripts to create exploratory and results oriented visualizations
    │       └── visualize.py
    │
    └── tox.ini            <- tox file with settings for running tox; see tox.readthedocs.io

Findings
============
I succeeded in developing a Machine Learning Model that captures the signal underlying the data and reproduces the correlation patterns identified during the exploratory phase of analysis. The accuracy of the final model is as follows: 

accuracy:			0.829  
precision:			0.926 
recall:				0.694

sensitivity:			0.694  
specificity:			0.95 
negative predictive value:	0.776

false positive rate:		0.05  
false negative rate:		0.306 
false discovery rate:		0.074
 
There were no business requirements given for this project, which is possibly the most important next step i.e. clearly define the model objectives. Without such objectives it is difficult to comment on the appropriateness of results. 

However, the 0.83 accuracy result on the holdout samples indicates that the model generalises well. This is further validated by the fact that the model has an extremely high accuracy without any regularisation.
 
The precision (positive predicted value) is extremely high 0.93 and could certainly be used for intervention purposes given the right business requirement i.e. false discovery rate is only 0.07. On the other hand, the sensitivity is lower at 0.7, which likewise could mean that the model is not appropriate for certain applications.  
  
The model is accurate enough to capture the directly proportionate relationship between several response variables as per the response curves shown above (in order of strength of association, based on response curve output):
* Thal, ca, slope, oldpeak, exang, restecg, chol, trestbps, cp, sex, age
and the inversely proportional relationship between:
* Thalach, fbs

Conclusion
===========
The model as applied to the validation dataset managed to capture the underlying signals in the data. We can therefore conclude that the model generalises well and that its accuracy is sufficiently high for this model to be used based on the features captured. The first next step would be to define a business requirement. The analysis showed that features strongly correlated with Coronary Artery disease in this dataset are in fact close to being proxy measures for the actual outcome:
 
* thal: Arteries found to be: 1. Normal 2. Reversible defect and 3. Fixed defect
* ca: Number of major vessels (0-3) coloured by fluoroscopy

are by nature close to the definition of Coronary Artery Disease itself. Care should therefore be taken when including these in a model where these measurements might not be available at the point of deployment of the model. 

References
===========

* Detrano, R., Janosi, A., Steinbrunn, W., Pfisterer, M., Schmid, J. J., Sandhu, S., Guppy, K. H., Lee, S., & Froelicher, V. (1989). International application of a new probability algorithm for the diagnosis of coronary artery disease. The American Journal of Cardiology, 64(5), 304–310. https://doi.org/10.1016/0002-9149(89)90524-9
* Patel, S. B. (2020). Heart Disease Prediction using Machine Learning and Data Mining. International Journal of Recent Technology and Engineering, 9(1), 21–219. https://doi.org/10.35940/ijrte.f9199.059120


--------

<p><small>Project based on the <a target="_blank" href="https://drivendata.github.io/cookiecutter-data-science/">cookiecutter data science project template</a>. #cookiecutterdatascience</small></p>
